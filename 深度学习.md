# 深度学习笔记

## 神经网络基础

参考博客：[神经网络浅讲：从神经元到深度学习](https://www.cnblogs.com/subconscious/p/5058741.html)

神经网络是一种==模拟人脑的神经网络==以期能够==实现类人工智能==的机器学习技术。人脑中的神经网络是一个非常复杂的组织。成人的大脑中估计有1000亿个神经元之多。

### 一. 前言

让我们来看一个经典的神经网络。这是一个包含三个层次的神经网络。红色的是**输入层**，绿色的是**输出层**，紫色的是**中间层**（也叫**隐藏层**）。输入层有3个输入单元，隐藏层有4个单元，输出层有2个单元。后文中，我们统一使用这种颜色来表达神经网络的结构。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151219151604318-1557737289.jpg)

​                                   图2 神经网络结构图

在开始介绍前，有一些知识可以先记在心里：

1. 设计一个神经网络时，==输入层与输出层==的节点数往往是固定的，中间层则可以自由指定；
2. 神经网络结构图中的拓扑与箭头代表着**预测**过程时数据的流向，跟**训练**时的数据流有一定的区别；
3. 结构图里的关键不是圆圈（代表“神经元”），而是连接线（代表“神经元”之间的连接）。每个连接线对应一个不同的**权重**（其值称为**权值**），这是需要训练得到的。

除了从左到右的形式表达的结构图，还有一种常见的表达形式是从下到上来表示一个神经网络。这时候，输入层在图的最下方。输出层则在图的最上方，如下图：

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151219174631693-775181930.jpg)

​                           图3 从下到上的神经网络结构图 

> 从左到右的表达形式以Andrew Ng和LeCun的文献使用较多，Caffe里使用的则是从下到上的表达。在本文中使用Andrew Ng代表的从左到右的表达形式。

　　下面从简单的神经元开始说起，一步一步介绍神经网络复杂结构的形成。



### 二. 神经元

#### 2.1 引子

对于神经元的研究由来已久，1904年生物学家就已经知晓了神经元的组成结构。

　　一个神经元通常具有多个**树突**，主要用来接受传入信息；而**轴突**只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做“**突触**”。

　　人脑中的神经元形状可以用下图做简单的说明：

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151229121248198-818698949.jpg)

​                                            图4 神经元

 

 　1943年，心理学家McCulloch和数学家Pitts参考了生物神经元的结构，发表了抽象的神经元模型MP。在下文中，我们会具体介绍神经元模型。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151219175550990-1730772549.jpg)                     ![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151219175600006-1000051743.jpg  )

​                  图5 Warren McCulloch（左）和 Walter Pitts（右） 

#### 2.2 结构 

　　神经元模型是一个包含输入，输出与计算功能的模型。输入可以类比为神经元的树突，而输出可以类比为神经元的轴突，计算则可以类比为细胞核。

> ==输入=树突   输出=轴突  计算=细胞核==
>
> 每条连线上都有一个对应的权重

　　下图是一个典型的神经元模型：包含有3个输入，1个输出，以及2个计算功能。

　　**注意：**中间的箭头线。这些线称为“连接”。每个上有一个“权值”。

非线性函数即是激活函数

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151219153856802-307732621.jpg)

​                                                     图6 神经元模型 

 

　　**连接**是神经元中最重要的东西。每一个连接上都有一个权重。

　　一个神经网络的训练算法就是让权重的值调整到最佳，以使得整个网络的预测效果最好。

　　我们使用a来表示输入，用w来表示权值。一个表示连接的有向箭头可以这样理解：在初端，传递的信号大小仍然是a，端中间有加权参数w，经过这个加权后的信号会变成a*w，因此在连接的末端，信号的大小就变成了a*w。

　　在其他绘图模型里，有向箭头可能表示的是值的不变传递。而在神经元模型里，每个有向箭头表示的是值的==加权传递==。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151219180614819-1652574235.jpg)

​                                                          图7 连接（connection） 

 

　　如果我们将神经元图中的所有变量用符号表示，并且写出输出的计算公式的话，就是下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151230201441792-1505283920.jpg)

​                                                         图8 神经元计算 

 

　　可见z是在输入和权值的线性加权和叠加了一个**函数g**的值。在MP模型里，函数g是sgn函数，也就是==取符号函数==。这个函数当输入大于0时，输出1，否则输出0。

　　下面对神经元模型的图进行一些扩展。首先将==sum函数与sgn函数合并到一个圆圈==里，代表神经元的内部计算。其次，把输入a与输出z写到连接线的左上方，便于后面画复杂的网络。最后说明，一个神经元可以引出多个代表输出的有向箭头，但值都是一样的。

　　神经元可以看作一个计算与存储单元。计算是神经元对其的输入进行计算功能。存储是神经元会暂存计算结果，并传递到下一层。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151230204036479-461440948.jpg)

​                                                     图9 神经元扩展 

 

　　当我们用“神经元”组成网络以后，描述网络中的某个“神经元”时，我们更多地会用“**单元**”（unit）来指代。同时由于神经网络的表现形式是一个有向图，有时也会用“**节点**”（node）来表达同样的意思。 

#### 2.3 效果 

　　神经元模型的使用可以这样理解：

　　我们有一个数据，称之为样本。样本有四个属性，其中三个属性已知，一个属性未知。我们需要做的就是通过三个已知属性**预测**未知属性。

　　具体办法就是使用神经元的公式进行计算。三个已知属性的值是a1，a2，a3，未知属性的值是z。z可以通过公式计算出来。

　　这里，已知的属性称之为**特征**，未知的属性称之为**目标**。假设特征与目标之间确实是线性关系，并且我们已经得到表示这个关系的权值w1，w2，w3。那么，我们就可以通过神经元模型预测新样本的目标。

#### 2.4 影响

　　1943年发布的MP模型，虽然简单，但已经建立了神经网络大厦的地基。但是，MP模型中，权重的值都是预先设置的，因此不能学习。

　　1949年心理学家Hebb提出了Hebb学习率，认为人脑神经细胞的**突触**（也就是连接）上的强度上可以变化的。于是计算科学家们开始考虑用调整权值的方法来让机器学习。这为后面的学习算法奠定了基础。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151219175742677-1785435491.gif)

​                                                 图10 Donald Olding Hebb 

 

　　尽管神经元模型与Hebb学习律都已诞生，但限于当时的计算机能力，直到接近10年后，第一个真正意义的神经网络才诞生。

> 神经网络学习的过程即是神经网络通过调整权重参数的过程

###  三. 单层神经网络（感知器）

#### 3.1 引子　　

　　1958年，计算科学家Rosenblatt提出了由==两层神经元==组成的神经网络。他给它起了一个名字--“感知器”（Perceptron）（有的文献翻译成“感知机”，下文统一用“感知器”来指代）。

　　感知器是当时首个可以学习的人工神经网络。Rosenblatt现场演示了其学习识别简单图像的过程，在当时的社会引起了轰动。

　　人们认为已经发现了智能的奥秘，许多学者和科研机构纷纷投入到神经网络的研究中。美国军方大力资助了神经网络的研究，并认为神经网络比“原子弹工程”更重要。这段时间直到1969年才结束，这个时期可以看作神经网络的第一次高潮(1958-1969年  第一次高潮)。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151221153812609-1784157068.jpg)

​                       图11 Rosenblat与感知器 

#### 3.2 结构

　　下面来说明感知器模型。

　　在原来MP模型的“输入”位置添加神经元节点，标志其为“输入单元”。其余不变，于是我们就有了下图：从本图开始，我们将权值w1, w2, w3写到“连接线”的中间。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151221151959015-1876891081.jpg)

​                                           图12 单层神经网络 

 

　　在“感知器”中，有两个层次。分别是输入层和输出层。输入层里的“输入单元”只负责传输数据，不做计算。输出层里的“输出单元”则需要对前面一层的输入进行计算。

　　我们把需要计算的层次称之为“计算层”，并把==拥有一个计算层的网络称之为“单层神经网络”==。有一些文献会按照网络拥有的层数来命名，例如把“感知器”称为两层神经网络。但在本文里，我们根据计算层的数量来命名。

　　假如我们要预测的目标不再是一个值，而是一个向量，例如[2,3]。那么可以在输出层再增加一个“输出单元”。

　　下图显示了带有两个输出单元的单层神经网络，其中输出单元z1的计算公式如下图。  

> 输出层输出单元的个数=向量的维度

  ![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151230204223917-579926148.jpg)

​                               图13 单层神经网络(Z1)

 

　　可以看到，z1的计算跟原先的z并没有区别。

　　我们已知一个神经元的输出可以向多个神经元传递，因此z2的计算公式如下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151230204258057-82126781.jpg)

​                          图14 单层神经网络(Z2)

 

　　可以看到，z2的计算中除了三个新的权值：w4，w5，w6以外，其他与z1是一样的。

　　整个网络的输出如下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151230204606760-610264555.jpg)

​                            图15 单层神经网络(Z1和Z2)

 

　　目前的表达公式有一点不让人满意的就是：w4，w5，w6是后来加的，很难表现出跟原先的w1，w2，w3的关系。

　　因此我们改用二维的下标，用w,x,y来表达一个权值。下标中的x代表后一层神经元的序号，而y代表前一层神经元的序号（序号的顺序从上到下）。

　　例如，w~1,2~代表后一层的第1个神经元与前一层的第2个神经元的连接的权值（这种标记方式参照了Andrew Ng的课件）。根据以上方法标记，我们有了下图。

> 第一个参数表示是后面的第几个神经元
>
> 第二个参数表示是前面的第几个神经元

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151230205437995-673856644.jpg)

​                                    图16 单层神经网络(扩展)

 

　　如果我们仔细看输出的计算公式，会发现这两个公式就是线性代数方程组。因此可以用矩阵乘法来表达这两个公式。

　　例如，输入的变量是[a1，a2，a3]^T^（代表由a1，a2，a3组成的列向量），用向量**a**来表示。方程的左边是[z1，z2]^T^，用向量**z**来表示。

　　系数则是矩阵**W**（2行3列的矩阵，排列形式与公式中的一样）。

　　于是，输出公式可以改写成：

​                                 g(**W** * **a**) = **z**;

 

　　这个公式就是神经网络中从前一层计算后一层的**矩阵运算。**

#### 3.3 效果

　　与神经元模型不同，感知器中的权值是通过训练得到的。因此，根据以前的知识我们知道，感知器类似一个**逻辑回归**模型，可以做线性分类任务。

　　我们可以用**决策分界**来形象的表达分类的效果。决策分界就是在二维的数据平面中划出一条直线，当数据的维度是3维的时候，就是划出一个平面，当数据的维度是n维时，就是划出一个n-1维的超平面。

　　下图显示了在二维平面中划出决策分界的效果，也就是感知器的分类效果。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151231073138323-962584420.png)

​                                   图17 单层神经网络（决策分界）

　　

#### 4.4 影响　

　　感知器只能做简单的线性分类任务。但是当时的人们热情太过于高涨，并没有人清醒的认识到这点。于是，当人工智能领域的巨擘Minsky指出这点时，事态就发生了变化。

　　Minsky在1969年出版了一本叫《Perceptron》的书，里面用详细的数学证明了感知器的弱点，尤其是感知器对XOR（异或）这样的简单分类任务都无法解决。

　　Minsky认为，如果将计算层增加到两层，计算量则过大，而且没有有效的学习算法。所以，他认为研究更深层的网络是没有价值的。

　　由于Minsky的巨大影响力以及书中呈现的悲观态度，让很多学者和实验室纷纷放弃了神经网络的研究。神经网络的研究陷入了冰河期。这个时期又被称为“AI winter”。

　　接近10年以后，对于两层神经网络的研究才带来神经网络的复苏。

 

### 四. 两层神经网络（多层感知器）

#### 4.1 引子

　　两层神经网络是本文的重点，因为正是在这时候，神经网络开始了大范围的推广与使用。

　　Minsky说过单层神经网络无法解决异或问题。但是当增加一个计算层以后，两层神经网络不仅可以解决异或问题，而且具有非常好的非线性分类效果。不过两层神经网络的计算是一个问题，没有一个较好的解法。

　　1986年，Rumelhar和Hinton等人提出了==反向传播==（Back propagation，BP）算法，解决了两层神经网络所需要的复杂计算量问题，从而带动了业界使用两层神经网络研究的热潮。目前，大量的教授神经网络的教材，都是重点介绍两层（带一个隐藏层）神经网络的内容。 

#### 4.2 结构

　　两层神经网络除了包含一个输入层，一个输出层以外，还增加了一个中间层。此时，中间层和输出层都是计算层。我们扩展上节的单层神经网络，在右边新加一个层次（只含有一个节点）。

　　现在，我们的权值矩阵增加到了两个，我们用上标来区分不同层次之间的变量。

　　例如a~x~^(y)^代表第y层的第x个节点。z1，z2变成了a~1~^(2)^，a~2~^(2)^。下图给出了a~1~^(2)^，a~2~^(2)^的计算公式。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151222164731249-360921014.jpg)

​                               图20 两层神经网络（中间层计算）

 

　　计算最终输出z的方式是利用了中间层的a~1~^(2)^，a~2~^(2)^和第二个权值矩阵计算得到的，如下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151222171056156-387680541.jpg)

​                             图21 两层神经网络（输出层计算）

 

　　假设我们的预测目标是一个向量，那么与前面类似，只需要在“输出层”再增加节点即可。

　　我们使用向量和矩阵来表示层次中的变量。a^(1)^，a^(2)^，**z**是网络中传输的向量数据。**W**(1)和**W**(2)是网络的矩阵参数。如下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151222171328140-1303075636.jpg)

图22 两层神经网络（向量形式）

 

　　使用矩阵运算来表达整个计算公式的话如下：

​                         g(**W**^(1)^ * **a**^(1)^) = **a**^(2)^; 

​                         g(**W**^(2)^ * **a**^(2)^) = **z**;

　　由此可见，使用矩阵运算来表达是很简洁的，而且也不会受到节点数增多的影响（无论有多少节点参与运算，乘法两端都只有一个变量）。因此神经网络的教程中大量使用矩阵运算来描述。

　　需要说明的是，至今为止，我们对神经网络的结构图的讨论中都没有提到偏置节点（bias unit）。事实上，这些节点是默认存在的。它本质上是一个只含有存储功能，且存储值永远为1的单元。在神经网络的每个层次中，除了输出层以外，都会含有这样一个偏置单元。正如线性回归模型与逻辑回归模型中的一样。

　　偏置单元与后一层的所有节点都有连接，我们设这些参数值为向量**b**，称之为偏置。如下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151226111144687-604911384.jpg)

​                             图23 两层神经网络（考虑偏置节点）

 

　　可以看出，偏置节点很好认，因为其没有输入（前一层中没有箭头指向它）。有些神经网络的结构图中会把偏置节点明显画出来，有些不会。一般情况下，我们都不会明确画出偏置节点。 

　　在考虑了偏置以后的一个神经网络的矩阵运算如下：

 g(**W**^(1)^ * **a**^(1)^ + **b**^(1)^) = **a**^(2)^; 

g(**W**^(2)^ * **a**^(2)^ + **b**^(2)^) = **z**;

 

　　需要说明的是，在两层神经网络中，我们不再使用sgn函数作为函数g，而是使用平滑函数sigmoid作为函数g。我们把函数g也称作激活函数（activation function）。

　　事实上，==神经网络的本质就是通过参数与激活函数来拟合特征与目标之间的真实函数关系==。初学者可能认为画神经网络的结构图是为了在程序中实现这些圆圈与线，但在一个神经网络的程序中，既没有“线”这个对象，也没有“单元”这个对象。实现一个神经网络最需要的是线性代数库。

#### 4.3  效果

　　与单层神经网络不同。理论证明，两层神经网络可以无限逼近任意连续函数。

　　这是什么意思呢？也就是说，面对复杂的非线性分类任务，两层（带一个隐藏层）神经网络可以分类的很好。

　　下面就是一个例子，红色的线与蓝色的线代表数据。而红色区域和蓝色区域代表由神经网络划开的区域，两者的分界线就是决策分界。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151231073619073-461403542.png)

​                          图24 两层神经网络（决策分界）

　　

　　可以看到，这个两层神经网络的决策分界是非常平滑的曲线，而且分类的很好。有趣的是，前面已经学到过，单层网络只能做线性分类任务。而两层神经网络中的后一层也是线性分类层，应该只能做线性分类任务。为什么两个线性分类任务结合就可以做非线性分类任务？

　　我们可以把输出层的决策分界单独拿出来看一下。就是下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151231074314604-2050732128.png)

​                               图25 两层神经网络（空间变换）

 

　　可以看到，输出层的决策分界仍然是直线。关键就是，从输入层到隐藏层时，数据发生了空间变换。也就是说，两层神经网络中，隐藏层对原始的数据进行了一个空间变换，使其可以被线性分类，然后输出层的决策分界划出了一个线性分类分界线，对其进行分类。

　　这样就导出了两层神经网络可以做非线性分类的关键--隐藏层。联想到我们一开始推导出的矩阵公式，我们知道，矩阵和向量相乘，本质上就是对向量的坐标空间进行一个变换。==因此，隐藏层的参数矩阵的作用就是使得数据的原始坐标空间从线性不可分，转换成了线性可分==。

　　两层神经网络通过两层的线性模型模拟了数据内真实的非线性函数。因此，多层的神经网络的本质就是复杂函数拟合。

　　下面来讨论一下隐藏层的节点数设计。在设计一个神经网络时，==输入层的节点数需要与特征的维度匹配==，==输出层的节点数要与目标的维度匹配==。而中间层的节点数，却是由设计者指定的。因此，“自由”把握在设计者的手中。但是，节点数设置的多少，却会影响到整个模型的效果。如何决定这个自由层的节点数呢？目前业界没有完善的理论来指导这个决策。一般是根据经验来设置。较好的方法就是预先设定几个可选值，通过切换这几个值来看整个模型的预测效果，选择效果最好的值作为最终选择。这种方法又叫做Grid Search（网格搜索）。

　　了解了两层神经网络的结构以后，我们就可以看懂其它类似的结构图。例如EasyPR字符识别网络架构（下图）。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151226122337406-1923048422.jpg)

​                                 图26 EasyPR字符识别网络

 

　　EasyPR使用了字符的图像去进行字符文字的识别。输入是120维的向量。输出是要预测的文字类别，共有65类。根据实验，我们测试了一些隐藏层数目，发现当值为40时，整个网络在测试集上的效果较好，因此选择网络的最终结构就是120，40，65。

#### 4.4 训练

　　在Rosenblat提出的感知器模型中，模型中的参数可以被训练，但是使用的方法较为简单，并没有使用目前机器学习中通用的方法，这导致其扩展性与适用性非常有限。从两层神经网络开始，神经网络的研究人员开始使用机器学习相关的技术进行神经网络的训练。例如用大量的数据（1000-10000左右），使用算法进行优化等等，从而使得模型训练可以获得性能与数据利用上的双重优势。

　　机器学习模型训练的目的，就是使得参数尽可能的与真实的模型逼近。具体做法是这样的。首先给所有参数赋上随机值。我们使用这些随机生成的参数值，来预测训练数据中的样本。样本的预测目标为y~p~，真实目标为y。那么，定义一个值loss，计算公式如下。

​                                loss = (y~p~ - y)^2^

 

　　这个值称之为**损失**（loss），我们的目标就是使对所有训练数据的损失和尽可能的小。

　　如果将先前的神经网络预测的矩阵公式带入到y~p~中（因为有z=y~p~），那么我们可以把损失写为关于参数（parameter）的函数，这个函数称之为**损失函数**（loss function）。下面的问题就是求：如何优化参数，能够让损失函数的值最小。

　　此时这个问题就被转化为一个优化问题。一个常用方法就是高等数学中的求导，但是这里的问题由于参数不止一个，求导后计算导数等于0的运算量很大，所以一般来说解决这个优化问题使用的是**梯度下降**算法。梯度下降算法每次计算参数在当前的梯度，然后让参数向着梯度的反方向前进一段距离，不断重复，直到梯度接近零时截止。一般这个时候，所有的参数恰好达到使损失函数达到一个最低值的状态。

　　在神经网络模型中，由于结构复杂，每次计算梯度的代价很大。因此还需要使用**反向传播**算法。反向传播算法是利用了神经网络的结构进行的计算。不一次计算所有参数的梯度，而是从后往前。首先计算输出层的梯度，然后是第二个参数矩阵的梯度，接着是中间层的梯度，再然后是第一个参数矩阵的梯度，最后是输入层的梯度。计算结束以后，所要的两个参数矩阵的梯度就都有了。

　　反向传播算法可以直观的理解为下图。梯度的计算从后往前，一层层反向传播。前缀E代表着相对导数的意思。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151229120754198-2003498733.jpg)

​                                   图27 反向传播算法

反向传播算法的启示是数学中的**链式法则**。在此需要说明的是，尽管早期神经网络的研究人员努力从生物学中得到启发，但从BP算法开始，研究者们更多地从数学上寻求问题的最优解。不再盲目模拟人脑网络是神经网络研究走向成熟的标志。正如科学家们可以从鸟类的飞行中得到启发，但没有必要一定要完全模拟鸟类的飞行方式，也能制造可以飞天的飞机。

　　优化问题只是训练中的一个部分。机器学习问题之所以称为学习问题，而不是优化问题，就是因为它不仅要求数据在训练集上求得一个较小的误差，在测试集上也要表现好。因为模型最终是要部署到没有见过训练数据的真实场景。提升模型在测试集上的预测效果的主题叫做**泛化**（generalization），相关方法被称作==正则化==（regularization）。神经网络中常用的泛化技术有**权重衰减**等。

#### 4.5 影响

　　两层神经网络在多个地方的应用说明了其效用与价值。10年前困扰神经网络界的异或问题被轻松解决。神经网络在这个时候，已经可以发力于语音识别，图像识别，自动驾驶等多个领域。

　　历史总是惊人的相似，神经网络的学者们再次登上了《纽约时报》的专访。人们认为神经网络可以解决许多问题。就连娱乐界都开始受到了影响，当年的《终结者》电影中的阿诺都赶时髦地说一句：我的CPU是一个神经网络处理器，一个会学习的计算机。

　　但是神经网络仍然存在若干的问题：尽管使用了BP算法，一次神经网络的训练仍然耗时太久，而且困扰训练优化的一个问题就是局部最优解问题，这使得神经网络的优化较为困难。同时，隐藏层的节点数需要调参，这使得使用不太方便，工程和研究人员对此多有抱怨。

　　90年代中期，由Vapnik等人发明的SVM（Support Vector Machines，支持向量机）算法诞生，很快就在若干个方面体现出了对比神经网络的优势：无需调参；高效；全局最优解。基于以上种种理由，SVM迅速打败了神经网络算法成为主流。

### 五. 多层神经网络（深度学习）

#### 5.1 引子　　

　　2006年，Hinton在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。与传统的训练方式不同，“深度信念网络”有一个“**预训练**”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“**微调**”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。他给多层神经网络相关的学习方法赋予了一个新名词——“**深度学习**”。

 　很快，深度学习在语音识别领域暂露头角。接着，2012年，深度学习技术又在图像识别领域大展拳脚。Hinton与他的学生在ImageNet竞赛中，用多层的卷积神经网络成功地对包含一千类别的一百万张图片进行了训练，取得了分类错误率15%的好成绩，这个成绩比第二名高了近11个百分点，充分证明了多层神经网络识别效果的优越性。

#### 5.2 结构

　　我们延续两层神经网络的方式来设计一个多层神经网络。

　　在两层神经网络的输出层后面，继续添加层次。原来的输出层变成中间层，新加的层次成为新的输出层。所以可以得到下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151224204339234-1994620313.jpg)

​                               图30 多层神经网络

 

　　依照这样的方式不断添加，我们可以得到更多层的多层神经网络。公式推导的话其实跟两层神经网络类似，使用矩阵运算的话就仅仅是加一个公式而已。

　　在已知输入**a**^(1)^，参数**W**^(1)^，**W**^(2)^，**W**^(3)^的情况下，输出**z**的推导公式如下：

​                  g(**W**^(1)^ * **a**^(1)^) = **a**^(2)^; 

​                  g(**W**^(2)^ * **a**^(2)^) = **a**^(3)^;

​                  g(**W**^(3)^ * **a**^(3)^) = **z**;

 

　　多层神经网络中，输出也是按照一层一层的方式来计算。从最外面的层开始，算出所有单元的值以后，再继续计算更深一层。只有当前层所有单元的值都计算完毕以后，才会算下一层。有点像计算向前不断推进的感觉。所以这个过程叫做“正向传播”。

　　下面讨论一下多层神经网络中的参数。

　　首先我们看第一张图，可以看出**W**^(1)^中有6个参数，**W**^(2)^中有4个参数，**W**^(3)^中有6个参数，所以整个神经网络中的参数有16个（这里我们不考虑偏置节点，下同）。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151224212531484-570745053.jpg) 

​                         图31 多层神经网络（较少参数）

 

　　假设我们将中间层的节点数做一下调整。第一个中间层改为3个单元，第二个中间层改为4个单元。

　　经过调整以后，整个网络的参数变成了33个。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151224213620234-1075501325.jpg) 

​                       图32 多层神经网络（较多参数）

 

　　虽然层数保持不变，但是第二个神经网络的参数数量却是第一个神经网络的接近两倍之多，从而带来了更好的表示（represention）能力。表示能力是多层神经网络的一个重要性质，下面会做介绍。

　　在==参数一致的情况下，我们也可以获得一个“更深”的网络==。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151224213703109-813423001.jpg) 

​                           图33 多层神经网络（更深的层次）

 

　　上图的网络中，虽然参数数量仍然是33，但却有4个中间层，是原来层数的接近两倍。这意味着一样的参数数量，可以用更深的层次去表达。

#### 5.3 效果

　　与两层层神经网络不同。多层神经网络中的层数增加了很多。

　　增加更多的层次有什么好处？更深入的表示特征，以及更强的函数模拟能力。

　　更深入的表示特征可以这样理解，随着网络的层数增加，每一层对于前一层次的抽象表示更深入。在神经网络中，每一层神经元学习到的是前一层神经元值的更抽象的表示。例如第一个隐藏层学习到的是“边缘”的特征，第二个隐藏层学习到的是由“边缘”组成的“形状”的特征，第三个隐藏层学习到的是由“形状”组成的“图案”的特征，最后的隐藏层学习到的是由“图案”组成的“目标”的特征。通过抽取更抽象的特征来对事物进行区分，从而获得更好的区分与分类能力。

　　关于逐层特征学习的例子，可以参考下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151231075103229-1126297331.png) 

​                                   图34 多层神经网络（特征学习）

　　更强的函数模拟能力是由于随着层数的增加，整个网络的参数就越多。而神经网络其实本质就是模拟特征与目标之间的真实关系函数的方法，更多的参数意味着其模拟的函数可以更加的复杂，可以有更多的**容量**（capcity）去拟合真正的关系。

　　通过研究发现，==在参数数量一样的情况下，更深的网络往往具有比浅层的网络更好的识别效率。==这点也在ImageNet的多次大赛中得到了证实。从2012年起，每年获得ImageNet冠军的深度神经网络的层数逐年增加，2015年最好的方法GoogleNet是一个多达22层的神经网络。

　　在最新一届的ImageNet大赛上，目前拿到最好成绩的MSRA团队的方法使用的更是一个深达152层的网络！关于这个方法更多的信息有兴趣的可以查阅ImageNet网站。

#### 5.4 训练

　　在单层神经网络时，我们使用的激活函数是sgn函数。到了两层神经网络时，我们使用的最多的是sigmoid函数。而到了多层神经网络时，通过一系列的研究发现，ReLU函数在训练多层神经网络时，更容易收敛，并且预测性能更好。因此，目前在深度学习中，最流行的非线性函数是ReLU函数。ReLU函数不是传统的非线性函数，而是分段线性函数。其表达式非常简单，就是y=max(x,0)。简而言之，在x大于0，输出就是输入，而在x小于0时，输出就保持为0。这种函数的设计启发来自于生物神经元对于激励的线性响应，以及当低于某个阈值后就不再响应的模拟。

　　在多层神经网络中，训练的主题仍然是优化和泛化。当使用足够强的计算芯片（例如GPU图形加速卡）时，梯度下降算法以及反向传播算法在多层神经网络中的训练中仍然工作的很好。目前学术界主要的研究既在于开发新的算法，也在于对这两个算法进行不断的优化，例如，增加了一种带动量因子（momentum）的梯度下降算法。　

　　在深度学习中，泛化技术变的比以往更加的重要。这主要是因为神经网络的层数增加了，参数也增加了，表示能力大幅度增强，很容易出现**过拟合现象**。因此正则化技术就显得十分重要。目前，==Dropout技术，以及数据扩容（Data-Augmentation）技术==是目前使用的最多的正则化技术。

> 使用正则化技术(dropout)解决过拟合的现象
>
> 每个神经元接收来自上一层的信号输入，使用一定的加和规则将所有的信号输入汇聚到一起，并使用**激活函数**将输入信号激活为输出信号，再将信号传递到下一层。
>
> 影响神经网络表现能力的主要因素有神经网络的层数、神经元的个数、神经元之间的连接方式以及神经元所采用的激活函数。

#### 5.5 影响

　　目前，深度神经网络在人工智能界占据统治地位。但凡有关人工智能的产业报道，必然离不开深度学习。神经网络界当下的四位引领者除了前文所说的Ng，Hinton以外，还有CNN的发明人Yann Lecun，以及《Deep Learning》的作者Bengio。

　　前段时间一直对人工智能持谨慎态度的马斯克，搞了一个[OpenAI项目](http://news.cnblogs.com/n/534878/)，邀请Bengio作为高级顾问。马斯克认为，人工智能技术不应该掌握在大公司如Google，Facebook的手里，更应该作为一种开放技术，让所有人都可以参与研究。马斯克的这种精神值得让人敬佩。

　　多层神经网络的研究仍在进行中。现在最为火热的研究技术包括RNN，LSTM等，研究方向则是图像理解方面。图像理解技术是给计算机一幅图片，让它用语言来表达这幅图片的意思。ImageNet竞赛也在不断召开，有更多的方法涌现出来，刷新以往的正确率。

### 六. 回顾

#### 6.1 影响　　

　　我们回顾一下神经网络发展的历程。神经网络的发展历史曲折荡漾，既有被人捧上天的时刻，也有摔落在街头无人问津的时段，中间经历了数次大起大落。

　　从单层神经网络（感知器）开始，到包含一个隐藏层的两层神经网络，再到多层的深度神经网络，一共有三次兴起过程。详见下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151228170208120-1856567090.jpg) 

​                                       图36 三起三落的神经网络

 

　　上图中的顶点与谷底可以看作神经网络发展的高峰与低谷。图中的横轴是时间，以年为单位。纵轴是一个神经网络影响力的示意表示。如果把1949年Hebb模型提出到1958年的感知机诞生这个10年视为落下（没有兴起）的话，那么神经网络算是经历了“三起三落”这样一个过程，跟“小平”同志类似。俗话说，天将降大任于斯人也，必先苦其心志，劳其筋骨。经历过如此多波折的神经网络能够在现阶段取得成功也可以被看做是磨砺的积累吧。

　　历史最大的好处是可以给现在做参考。科学的研究呈现螺旋形上升的过程，不可能一帆风顺。同时，这也给现在过分热衷深度学习与人工智能的人敲响警钟，因为这不是第一次人们因为神经网络而疯狂了。1958年到1969年，以及1985年到1995，这两个十年间人们对于神经网络以及人工智能的期待并不现在低，可结果如何大家也能看的很清楚。

　　因此，冷静才是对待目前深度学习热潮的最好办法。如果因为深度学习火热，或者可以有“钱景”就一窝蜂的涌入，那么最终的受害人只能是自己。神经网络界已经两次有被人们捧上天了的境况，相信也对于捧得越高，摔得越惨这句话深有体会。因此，神经网络界的学者也必须给这股热潮浇上一盆水，不要让媒体以及投资家们过分的高看这门技术。很有可能，三十年河东，三十年河西，在几年后，神经网络就再次陷入谷底。根据上图的历史曲线图，这是很有可能的。

#### 6.2 效果　　

　　下面说一下神经网络为什么能这么火热？简而言之，就是其学习效果的强大。随着神经网络的发展，其表示性能越来越强。

　　从单层神经网络，到两层神经网络，再到多层神经网络，下图说明了，随着网络层数的增加，以及激活函数的调整，神经网络所能拟合的决策分界平面的能力。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151228134016120-1091351096.jpg) 

​                                图37 表示能力不断增强

 

　　可以看出，随着层数增加，其非线性分界拟合能力不断增强。图中的分界线并不代表真实训练出的效果，更多的是示意效果。

　　神经网络的研究与应用之所以能够不断地火热发展下去，与其强大的函数拟合能力是分不开关系的。

#### 6.3 外因　　

　　当然，光有强大的内在能力，并不一定能成功。一个成功的技术与方法，不仅需要内因的作用，还需要时势与环境的配合。神经网络的发展背后的外在原因可以被总结为：更强的计算性能，更多的数据，以及更好的训练方法。只有满足这些条件时，神经网络的函数拟合能力才能得已体现，见下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151228170149135-2107087462.jpg) 

​                                 图38 发展的外在原因

　　之所以在单层神经网络年代，Rosenblat无法制作一个双层分类器，就在于当时的计算性能不足，Minsky也以此来打压神经网络。但是Minsky没有料到，仅仅10年以后，计算机CPU的快速发展已经使得我们可以做两层神经网络的训练，并且还有快速的学习算法BP。

　　但是在两层神经网络快速流行的年代。更高层的神经网络由于计算性能的问题，以及一些计算方法的问题，其优势无法得到体现。直到2012年，研究人员发现，用于高性能计算的图形加速卡（GPU）可以极佳地匹配神经网络训练所需要的要求：高并行性，高存储，没有太多的控制需求，配合预训练等算法，神经网络才得以大放光彩。

　　互联网时代，大量的数据被收集整理，更好的训练方法不断被发现。所有这一切都满足了多层神经网络发挥能力的条件。

　　“时势造英雄”，正如Hinton在2006年的论文里说道的

　　“**... provided that computers were fast enough, data sets were big enough, and the initial weights were close enough to a good solution. All three conditions are now satisfied.**”，

　　外在条件的满足也是神经网络从神经元得以发展到目前的深度神经网络的重要因素。

　　除此以外，一门技术的发扬没有“伯乐”也是不行的。在神经网络漫长的历史中，正是由于许多研究人员的锲而不舍，不断钻研，才能有了现在的成就。前期的Rosenblat，Rumelhart没有见证到神经网络如今的流行与地位。但是在那个时代，他们为神经网络的发展所打下的基础，却会永远流传下去，不会退色。

 

### 七. 展望

#### 7.1 量子计算

　　回到我们对神经网络历史的讨论，根据历史趋势图来看，神经网络以及深度学习会不会像以往一样再次陷入谷底？作者认为，这个过程可能取决于量子计算机的发展。

　　根据一些最近的研究发现，人脑内部进行的计算可能是类似于量子计算形态的东西。而且目前已知的最大神经网络跟人脑的神经元数量相比，仍然显得非常小，仅不及1%左右。所以未来真正想实现人脑神经网络的模拟，可能需要借助量子计算的强大计算能力。

　　各大研究组也已经认识到了量子计算的重要性。谷歌就在开展量子计算机D-wave的研究，希望用量子计算来进行机器学习，并且在前段时间有了突破性的[进展](http://news.cnblogs.com/n/535307/)。国内方面，阿里和中科院合作成立了[量子计算实验室](http://news.sciencenet.cn/htmlnews/2015/7/323963.shtm)，意图进行量子计算的研究。

　　如果量子计算发展不力，仍然需要数十年才能使我们的计算能力得以突飞猛进的发展，那么缺少了强大计算能力的神经网络可能会无法一帆风顺的发展下去。这种情况可以类比为80-90年时期神经网络因为计算能力的限制而被低估与忽视。假设量子计算机真的能够与神经网络结合，并且助力真正的人工智能技术的诞生，而且量子计算机发展需要10年的话，那么神经网络可能还有10年的发展期。直到那时期以后，神经网络才能真正接近实现AI这一目标。



 

## CNN

​		卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络（Feedforward Neural Networks），是深度学习（deep learning）的代表算法之一。
​		其主要结构分为输入层、隐含层、输出层。
​		对于卷积神经网络而言，其输入层、输出层与平常的卷积神经网络无异。但其隐含层可以分为三个部分，分别是卷积层（对输入数据进行特征提取）、池化层（特征选择和信息过滤）、全连接层（等价于传统前馈神经网络中的隐含层）。

### 1、卷积层

卷积将输入图像放进一组卷积滤波器，每个滤波器激活图像中的某些特征。
假设一副黑白图像为5*5的大小，像这样：

![示例](https://img-blog.csdnimg.cn/20190725222822882.png#pic_center)

利用如下卷积核(kernel)进行卷积：

![示例](https://img-blog.csdnimg.cn/20190725222809675.png#pic_center)



卷积结果为：

![img](https://img-blog.csdnimg.cn/20190725222859770.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70#pic_center)

**卷积的过程，引用一幅形象的图：** 下面灰色的一部分可以看作是卷积核
![img](https://img-blog.csdnimg.cn/20190528144300189.gif)

大多数卷积的过程中，形状是没有发生变化的。那么他需要多少个weight 和 bias呢？

以图片中的例子给大家逐步解释：
以卷积核为单位： 卷积核形状为 [3, 3] 时，每一次卷积就需要 [3, 3]个w, [0] 个b
以图为单位： 图的形状为[5, 5], 每个点都需要 [3, 3]的卷积神经，需要[5, 5, 3, 3]个w, [0] 个 b
==存在2个图片为输入==： 需要[5, 5, 3, 3, 2]个w, [0] 个 b
==存在4个特征图为输出==： 需要[5, 5, 3, 3, 2, 4]个w, [4] 个 b

==结论：w和输入的形状是强相关的，b和输出的形状相关。==



卷积过程可以提取特征，卷积神经网络是根据特征来完成分类的。
在tensorflow中，卷积层的重要函数是：
tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)，其中：
1、input是输入量，shape是[batch, height, width, channels]。；
2、filter是使用的卷积核；
3、strides是步长，其格式[1,step,step,1]，step指的是在图像卷积的每一维的步长；
4、padding：string类型的量，只能是"SAME","VALID"其中之一，SAME表示卷积前后图像面积不变。

[padding的详细介绍](https://zhuanlan.zhihu.com/p/36278093)



**padding 存在的意义在于**

> 为了不丢弃原图信息
> 为了保持feature map 的大小与原图一致
> 为了让更深层的layer的input依旧保持有足够大的信息量
> 为了实现上述目的，且不做多余的事情，padding出来的pixel的值都是0，不存在噪音问题。



**padding**

> 为什么需要做Padding？
> 为什么conv-layer 之后需要加pooling_layer?
> 如果只有conv-layer会怎样？
> 使用pooling layer能解决什么问题？
> 为什么max_pooling 要优于strided convolution 和average_pooling?
> 最有downsampling的组合是什么？
> 如何计算feature map的长宽或filter的长宽？
> 怎样的情况下，设定padding=1或2或其他？
> 为什么filter长宽要用奇数值？

![preview](http://cdn.nefu-yzk.top/img/v2-b9d13f945fe34c8d0e064cde5db3a6d4_r.jpg)

![](http://cdn.nefu-yzk.top/img/v2-b9d13f945fe34c8d0e064cde5db3a6d4_r.jpg)

完整的计算feature map 长与宽的公式 = ![[公式]](https://www.zhihu.com/equation?tex=%28n+%2B+2p+-+f%29%2Fs+%2B1)

n 原图片（input）长与宽，p为padding，f是filter的长与宽， s是stride值

### 2、池化层

​      池化层主要用来做特征选择和信息过滤的，一般选择最大池化maxpool，即是从池化窗口中选择值最大的那个值，保留了最大特征

池化层用于在卷积层进行特征提取后，输出的特征图会被传递至池化层进行特征选择和信息过滤。
常见的池化是最大池化，最大池化指的是取出这些被卷积后的数据的最大值，就是取出其最大特征。
假设其池化窗口为2X2，步长为2。
原图像为：

![示例](https://img-blog.csdnimg.cn/20190725231414495.png#pic_center)

池化后为：

![示例](https://img-blog.csdnimg.cn/20190725231428345.png#pic_center)

**池化的过程**

![img](https://img-blog.csdnimg.cn/20190528144924221.gif)

在tensorflow中，池化层的重要函数是：
tf.nn.max_pool(value, ksize, strides, padding, data_format, name)
1、value：池化层的输入，一般池化层接在卷积层后面，shape是[batch, height, width, channels]。
2、ksize：池化窗口的大小，取一个四维向量，一般是[1, in_height, in_width, 1]。
3、strides：和卷积类似，窗口在每一个维度上滑动的步长，也是[1, stride,stride, 1]。
4、padding：和卷积类似，可以取’VALID’ 或者’SAME’。

### 3、具体实现代码

```python
def conv2d(x,W,step,pad):   #用于进行卷积，x为输入值，w为卷积核
    return tf.nn.conv2d(x,W,strides = [1,step,step,1],padding = pad)
    
def max_pool_2X2(x,step,pad):	#用于池化，x为输入值，step为步数
    # 池化的窗口为2x2
    return tf.nn.max_pool(x,ksize = [1,2,2,1],strides= [1,step,step,1],padding = pad)

def weight_variable(shape):		#用于获得W(卷积核)
    initial = tf.truncated_normal(shape,stddev = 0.1) #从截断的正态分布中输出随机值
    return tf.Variable(initial)

# y = w * x + b
# w 为权重比
# x 为输入的特征值
# b 为偏斜量
def bias_variable(shape):		#获得bias  偏置单元
    initial = tf.constant(0.1,shape=shape)  #生成普通值
    return tf.Variable(initial)

# activation_function 为激活函数
def add_layer(inputs,in_size,out_size,n_layer,activation_function = None,keep_prob = 1):
	#用于添加全连接层
    layer_name = 'layer_%s'%n_layer  # n_layer会填补在%s处
    with tf.name_scope(layer_name):
        with tf.name_scope("Weights"):
            Weights = tf.Variable(tf.truncated_normal([in_size,out_size],stddev = 0.1),name = "Weights")
            tf.summary.histogram(layer_name+"/weights",Weights)
        with tf.name_scope("biases"):
            biases = tf.Variable(tf.zeros([1,out_size]) + 0.1,name = "biases")
            tf.summary.histogram(layer_name+"/biases",biases)
        with tf.name_scope("Wx_plus_b"):
            Wx_plus_b = tf.matmul(inputs,Weights) + biases
            tf.summary.histogram(layer_name+"/Wx_plus_b",Wx_plus_b)
        if activation_function == None :
            outputs = Wx_plus_b 
        else:
            outputs = activation_function(Wx_plus_b)
        print(activation_function)
        outputs = tf.nn.dropout(outputs,keep_prob)
        tf.summary.histogram(layer_name+"/outputs",outputs)
        return outputs

def add_cnn_layer(inputs, in_z_dim, out_z_dim, n_layer, conv_step = 1, pool_step = 2, padding = "SAME"):
	#用于生成卷积层和池化层  卷积层之后便是池化层
    layer_name = 'layer_%s'%n_layer
    with tf.name_scope(layer_name):
        with tf.name_scope("Weights"):
            W_conv = weight_variable([5,5,in_z_dim,out_z_dim])
        with tf.name_scope("biases"):
            b_conv = bias_variable([out_z_dim])
        with tf.name_scope("conv"):
        #卷积层
            h_conv = tf.nn.relu(conv2d(inputs, W_conv, conv_step, padding)+b_conv) 
        with tf.name_scope("pooling"):
        #池化层
            h_pool = max_pool_2X2(h_conv, pool_step, padding)
    return h_pool

```

[TF：CNN里面的weight，bias，shape，从形状的角度理解整个过程](https://blog.csdn.net/weixin_40904071/article/details/90636096)

### 4、全部代码

```python
import tensorflow as tf 
from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets("MNIST_data",one_hot = "true")

def conv2d(x,W,step,pad):
    return tf.nn.conv2d(x,W,strides = [1,step,step,1],padding = pad)
    
def max_pool_2X2(x,step,pad):
    return tf.nn.max_pool(x,ksize = [1,2,2,1],strides= [1,step,step,1],padding = pad)

def weight_variable(shape):
    initial = tf.truncated_normal(shape,stddev = 0.1) #从截断的正态分布中输出随机值
    return tf.Variable(initial)

def bias_variable(shape):
    initial = tf.constant(0.1,shape=shape)  #生成普通值
    return tf.Variable(initial)

def add_layer(inputs,in_size,out_size,n_layer,activation_function = None,keep_prob = 1):
    layer_name = 'layer_%s'%n_layer
    with tf.name_scope(layer_name):
        with tf.name_scope("Weights"):
            Weights = tf.Variable(tf.truncated_normal([in_size,out_size],stddev = 0.1),name = "Weights")
            tf.summary.histogram(layer_name+"/weights",Weights)
        with tf.name_scope("biases"):
            biases = tf.Variable(tf.zeros([1,out_size]) + 0.1,name = "biases")
            tf.summary.histogram(layer_name+"/biases",biases)
        with tf.name_scope("Wx_plus_b"):
            Wx_plus_b = tf.matmul(inputs,Weights) + biases
            tf.summary.histogram(layer_name+"/Wx_plus_b",Wx_plus_b)
        if activation_function == None :
            outputs = Wx_plus_b 
        else:
            outputs = activation_function(Wx_plus_b)
        print(activation_function)
        outputs = tf.nn.dropout(outputs,keep_prob)
        tf.summary.histogram(layer_name+"/outputs",outputs)
        return outputs

def add_cnn_layer(inputs, in_z_dim, out_z_dim, n_layer, conv_step = 1, pool_step = 2, padding = "SAME"):
    layer_name = 'layer_%s'%n_layer
    with tf.name_scope(layer_name):
        with tf.name_scope("Weights"):
            W_conv = weight_variable([5,5,in_z_dim,out_z_dim])
        with tf.name_scope("biases"):
            b_conv = bias_variable([out_z_dim])
        with tf.name_scope("conv"):
            h_conv = tf.nn.relu(conv2d(inputs, W_conv, conv_step, padding)+b_conv) 
        with tf.name_scope("pooling"):
            h_pool = max_pool_2X2(h_conv, pool_step, padding)
    return h_pool
    
def compute_accuracy(x_data,y_data):
    global prediction
    y_pre = sess.run(prediction,feed_dict={xs:x_data,keep_prob:1})

    correct_prediction = tf.equal(tf.arg_max(y_data,1),tf.arg_max(y_pre,1))

    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))

    result = sess.run(accuracy,feed_dict = {xs:batch_xs,ys:batch_ys,keep_prob:1})
    return result



keep_prob = tf.placeholder(tf.float32)
xs = tf.placeholder(tf.float32,[None,784])
ys = tf.placeholder(tf.float32,[None,10])

x_image = tf.reshape(xs,[-1,28,28,1])

h_pool1 = add_cnn_layer(x_image, in_z_dim = 1, out_z_dim = 32, n_layer = "cnn1",)
h_pool2 = add_cnn_layer(h_pool1, in_z_dim = 32, out_z_dim = 64, n_layer = "cnn2",)

h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])

h_fc1_drop = add_layer(h_pool2_flat, 7*7*64, 1024, "layer1", activation_function = tf.nn.relu, keep_prob = keep_prob)
prediction = add_layer(h_fc1_drop, 1024, 10, "layer2", activation_function = tf.nn.softmax, keep_prob = 1)

with tf.name_scope("loss"):
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ys,logits = prediction),name = 'loss')
    tf.summary.scalar("loss",loss)
    
train = tf.train.AdamOptimizer(1e-4).minimize(loss)
init = tf.initialize_all_variables()

merged = tf.summary.merge_all()

with tf.Session() as sess:
    sess.run(init)
    write = tf.summary.FileWriter("logs/",sess.graph)
    for i in range(5000):
        batch_xs,batch_ys = mnist.train.next_batch(100)
        sess.run(train,feed_dict = {xs:batch_xs,ys:batch_ys,keep_prob:0.5})
        if i % 100 == 0:
            print(compute_accuracy(mnist.test.images,mnist.test.labels))

```

### 5、池化 -> 全连接 -> 输出

这个过程，引用一幅形象的图：

![img](https://img-blog.csdnimg.cn/20190528151549253.gif)





## RNN

神经网络可以当做是能够拟合任意函数的黑盒子，只要训练数据足够，给定特定的x，就能得到希望的y，结构图如下：

![img](https://pic2.zhimg.com/v2-dbd25d81a8537985345a3e46077931ed_b.jpg)

将神经网络模型训练好之后，在输入层给定一个x，通过网络之后就能够在输出层得到特定的y，但是传统的神经网络模型对数据量有很大的要求，但往往我们没有大量的数据去尝试。

RNN是用来处理那些前面的输入和后面的输入是有关系的任务。因此RNN能够更好的处理**序列**的信息。

> **比如我们理解一句话时，不能单独的与理解一个词，需要整体的来看待这句话，即分析这些词连接起来的整个序列；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。**
>
> **RNN虽然在理论上可以保留所有历史时刻的信息，但在实际使用时，信息的传递往往会因为时间间隔太长而逐渐衰减，传递一段时刻以后其信息的作用效果就大大降低了。因此，普通RNN对于信息的长期依赖问题没有很好的处理办法。**
>
> 现在假设有四个字，“我” “去” “吃” “饭”。我们可以对它们进行任意的排列组合。
> “我去吃饭”，表示的就是我要去吃饭了。
> “饭去吃我”，表示的就是饭成精了。
> “我吃去饭”，表示的我要去吃‘去饭’了。
> 不同的排列顺序会导致不同的语意，序列数据表示的就是按照一定顺序排列的序列，这种排列一般存在一定的意义

如：我吃苹果

​      显然，一个句子中，前一个单词其实对于当前单词的词性预测是有很大影响的，比如预测苹果的时候，由于前面的吃是一个动词，那么很显然苹果作为名词的概率就会远大于动词的概率，因为动词后面接名词很常见，而动词后面接动词很少见。

### 1、RNN结构

首先看一个简单的循环神经网络图，它由输入层、一个隐藏层和一个输出层组成：

![img](https://pic4.zhimg.com/v2-3884f344d71e92d70ec3c44d2795141f_b.jpg)

其中X是一个向量，它表示**输入层**的值（这里面没有画出来表示神经元节点的圆圈）；S是一个向量，它表示**隐藏层**的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；

U是输入层到隐藏层的**权重矩阵**，O也是一个向量，它表示**输出层**的值；V是隐藏层到输出层的**权重矩阵**。

**循环神经网络(RNN)**的**隐藏层**的值S不仅仅取决于当前这次的输入X，还取决于上一次**隐藏层**的值S。**权重矩阵** W就是**隐藏层**上一次的值作为这一次的输入的权重。

如果我们把上面的图展开，**循环神经网络**也可以画成下面这个样子：

![img](https://pic2.zhimg.com/v2-b0175ebd3419f9a11a3d0d8b00e28675_b.jpg)

现在看上去就比较清楚了，这个网络在t时刻接收到输入 ![[公式]](https://www.zhihu.com/equation?tex=x_%7Bt%7D) 之后，隐藏层的值是 ![[公式]](https://www.zhihu.com/equation?tex=s_%7Bt%7D) ，输出值是 ![[公式]](https://www.zhihu.com/equation?tex=o_%7Bt%7D) 。关键一点是， ![[公式]](https://www.zhihu.com/equation?tex=s_%7Bt%7D) 的值不仅仅取决于 ![[公式]](https://www.zhihu.com/equation?tex=x_%7Bt%7D) ，还取决于 ![[公式]](https://www.zhihu.com/equation?tex=s_%7Bt-1%7D) 。我们可以用下面的公式来表示**循环神经网络**的计算方法：

**用公式表示如下：**

![img](https://pic4.zhimg.com/v2-9524a28210c98ed130644eb3c3002087_b.jpg)

### 2、TF中RNN的相关函数

#### 2.1 tf.nn.rnn_cell.BasicLSTMCell

```python
tf.nn.rnn_cell.BasicRNNCell(
	num_units, 
	activation=None, 
	reuse=None, 
	name=None, 
	dtype=None, 
	**kwargs)
```

> num_units：RNN单元中的神经元数量，即输出神经元数量。
> activation：激活函数。
> reuse：描述是否在现有范围中重用变量。如果不为True，并且现有范围已经具有给定变量，则会引发错误。
> name：层的名称。
> dtype：该层的数据类型。
> kwargs：常见层属性的关键字命名属性，如trainable，当从get_config()创建cell 。

使用时可以定义为：

```python
RNN_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden_units,activation=tf.nn.tanh)
```

在定义完成后，可以进行状态初始化：

```python
_init_state =  RNN_cell.zero_state(batch_size,tf.float32)
```

#### 2.2 tf.nn.dynamic_rnn

```python
tf.nn.dynamic_rnn(
    cell,
    inputs,
    sequence_length=None,
    initial_state=None,
    dtype=None,
    parallel_iterations=None,
    swap_memory=False,
    time_major=False,
    scope=None
)
```

> cell：上文所定义的lstm_cell。
> inputs：RNN输入。如果time_major==false（默认），则必须是如下shape的tensor：[batch_size，max_time，…]或此类元素的嵌套元组。如果time_major==true，则必须是如下形状的tensor：[max_time，batch_size，…]或此类元素的嵌套元组。
> sequence_length：Int32/Int64矢量大小。用于在超过批处理元素的序列长度时复制通过状态和零输出。因此，它更多的是为了性能而不是正确性。
> initial_state：上文所定义的_init_state。
> dtype：数据类型。
> parallel_iterations：并行运行的迭代次数。那些不具有任何时间依赖性并且可以并行运行的操作将是。这个参数用时间来交换空间。值>>1使用更多的内存，但花费的时间更少，而较小的值使用更少的内存，但计算需要更长的时间。
> time_major：输入和输出tensor的形状格式。如果为真，这些张量的形状必须是[max_time，batch_size，depth]。如果为假，这些张量的形状必须是[batch_size，max_time，depth]。使用time_major=true会更有效率，因为它可以避免在RNN计算的开始和结束时进行换位。但是，大多数TensorFlow数据都是批处理主数据，因此默认情况下，此函数为False。
> scope：创建的子图的可变作用域；默认为“RNN”。

在RNN的最后，需要用该函数得出结果。

```python
outputs,states = tf.nn.dynamic_rnn(RNN_cell,X_in,initial_state = _init_state,time_major = False)
```

返回的是一个元组 (outputs, state)：

> ==outputs==：RNN的最后一层的输出，是一个tensor。如果为time_major== False，则它的shape为[batch_size,max_time,cell.output_size]。如果为time_major== True，则它的shape为[max_time,batch_size,cell.output_size]。
>
> ==states==：states是一个tensor。state是最终的状态，也就是序列中最后一个cell输出的状态。一般情况下states的形状为 [batch_size, cell.output_size]，但当输入的cell为BasicLSTMCell时，states的形状为[2，batch_size, cell.output_size ]，其中2也对应着LSTM中的cell state和hidden state。

整个RNN的定义过程为：

```python
def RNN(X,weights,biases):
    # X最开始的形状为(128 batch,28 steps,28 inputs)
    # 转化为(128 batch*28 steps,128 hidden)
    X = tf.reshape(X,[-1,n_inputs])
    # 经过乘法后结果为(128 batch*28 steps,256 hidden)
    X_in = tf.matmul(X,weights['in'])+biases['in']
    # 再次转化为(128 batch,28 steps,256 hidden) 
    X_in = tf.reshape(X_in,[-1,n_steps,n_hidden_units])
    
    RNN_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden_units,activation=tf.nn.tanh)
    _init_state =  RNN_cell.zero_state(batch_size,tf.float32)

    outputs,states = tf.nn.dynamic_rnn(RNN_cell,X_in,initial_state = _init_state,time_major = False)

    results = tf.matmul(states,weights['out'])+biases['out']

    return results

```



### 3、RNN的总结

![img](https://pic3.zhimg.com/v2-9e50e23bd3dff0d91b0198d0e6b6429a_b.jpg)

 参考博客：

[一文搞懂RNN](https://zhuanlan.zhihu.com/p/30844905)

[利用tensorflow构建循环神经网络（RNN）](https://blog.csdn.net/weixin_44791964/article/details/98476021?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-0.highlightwordscore&spm=1001.2101.3001.4242.1)



## LSTM

RNN存在的最大问题是，当w1、w2、w3这些值小于0时，如果一句话够长，那么其在神经网络进行反向传播与前向传播时，存在梯度消失的问题。

0.9^25^=0.07，如果一句话有20到30个字，那么第一个字的隐含层输出传递到最后，将会变为原来的0.07倍，相比于最后一个字的影响，大大降低。

长短时记忆网络就是==为了解决梯度消失==的问题出现的。

长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是==为了解决长序列训练过程中的梯度消失和梯度爆炸问题==。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。

### 1、LSTM的结构

原始RNN的隐藏层只有一个状态h，从头传递到尾，它对于短期的输入非常敏感。

如果我们再增加一个状态c，让它来保存长期的状态，问题就可以解决了。

LSTM结构（图右）和普通RNN的主要输入输出区别如下所示。

![img](https://pic4.zhimg.com/v2-e4f9851cad426dfe4ab1c76209546827_b.jpg)

相比RNN只有一个传递状态 ![[公式]](https://www.zhihu.com/equation?tex=h%5Et+) ，LSTM有两个传输状态，一个 ![[公式]](https://www.zhihu.com/equation?tex=c%5Et) （cell state），和一个 ![[公式]](https://www.zhihu.com/equation?tex=h%5Et) （hidden state）。（Tips：RNN中的 ![[公式]](https://www.zhihu.com/equation?tex=h%5Et) 对于LSTM中的 ![[公式]](https://www.zhihu.com/equation?tex=c%5Et) ）

其中对于传递下去的 ![[公式]](https://www.zhihu.com/equation?tex=c%5Et) 改变得很慢，通常输出的 ![[公式]](https://www.zhihu.com/equation?tex=c%5Et) 是上一个状态传过来的 ![[公式]](https://www.zhihu.com/equation?tex=c%5E%7Bt-1%7D) 加上一些数值。

而 ![[公式]](https://www.zhihu.com/equation?tex=h%5Et) 则在不同节点下往往会有很大的区别。

### 2、LSTM的总结

RNN有一定的记忆能力，不幸的是RNN只能保留短期记忆，即是如果序列信息过长时，前面的信息对后面的信息作用效果微乎其微

LSTM相比于RNN多了门这种机制，门是用来决定信息如何保留的小开关，数值介于0-1之间  0是完全舍弃，1是完全保留

LSTM中有三个门：遗忘门、输入门、输出门

遗忘门：决定了小盒子里要保留多少原有信息，也就是丢掉哪些不重要的信息。

输入门：决定了当前网络信息中有多少要被保存到小盒子中，也就是记住哪些东西

输出门：决定了多大程度的输出小盒子中的信息

![image-20211113154336796](http://cdn.nefu-yzk.top/img/image-20211113154336796.png)

经过改造后的小盒子，既能通过输入门对当前的网络状态有所了解，又能利用遗忘门留下过往的重要信息

![](http://cdn.nefu-yzk.top/img/image-20211113154520024.png)

LSTM：Long Short Term Memory  长短时记忆模型

通过改变小盒子的结构，LSTM还有很多变体，如MGU、SRU、GRU(门控循环单元)

GRU只有两个门(更新门和重置门)，其中更新门是遗忘门和输入门的结合体，决定丢弃哪些旧信息(遗忘门)，添加哪些新信息(输入门)

![image-20211113155833995](http://cdn.nefu-yzk.top/img/image-20211113155833995.png)

重置门决定写入多少上一时刻网络的状态，用来捕捉短期记忆

![image-20211113155908920](http://cdn.nefu-yzk.top/img/image-20211113155908920.png)



## GNN

通常神经网络的输入是图片、字符或是声音信号

图神经网络输入的图，并非是指图片，而是指由节点和边组成的图

图中的节点代表了实体，包含了它们的属性

边则描述了实体之间的关系

![image-20211113162841601](http://cdn.nefu-yzk.top/img/image-20211113162841601.png)



图能表述的范围极其的广泛

​     大到社交网络中人与他们的关系

![image-20211113162925806](http://cdn.nefu-yzk.top/img/image-20211113162925806.png)



![image-20211113163016891](http://cdn.nefu-yzk.top/img/image-20211113163016891.png)

将重点放在节点上，我们可以预测社交网络中用户的标签，判断用户是否为恶意账户

![image-20211113163133746](http://cdn.nefu-yzk.top/img/image-20211113163133746.png)



对边进行预测，推荐系统就能找到适合用户的商品

![image-20211113163216705](http://cdn.nefu-yzk.top/img/image-20211113163216705.png)



图能解决的问题很多

![image-20211113163350936](http://cdn.nefu-yzk.top/img/image-20211113163350936.png)

## BP

真正决定神经网络好不好用的决定因素是神经元之间连接的权重和神经元的阈值 

![image-20211113163658291](http://cdn.nefu-yzk.top/img/image-20211113163658291.png)

大部分时间我们都在使用反向传播BP来进行确定这些数值

BP算法的思想即根据网络输出的答案与正确答案之间的误差不断调整网络的参数，如果误差值为负，则提高权重，否则降低权重

调整的程度受一定比率即"学习率"的制约，它像一个旋钮，用来控制参数调整程度的高低。

在一次次输入和反向调整中，网络就能得到不错的输出

过拟合即是在训练数据上结果很好，但是却无法正确的识别测试数据

![image-20211113163935053](http://cdn.nefu-yzk.top/img/image-20211113163935053.png)

提前停止：可以将数据集划分为训练集和测试集，如果在训练集上的误差降低，而在测试集上的误差升高时，说明了出现了过拟合的现象，即可结束训练

![image-20211113164451006](http://cdn.nefu-yzk.top/img/image-20211113164451006.png)



## GBDT



## TransFormer

例子：处理机器翻译时，用什么模型会比较好？

RNN是个不错的选项，单词的先后顺序会影响句子的意义，擅长捕捉序列关系的它非常的合适，但是对于翻译而言，句子间的单词数量并非是一一对应的。

![image-20211113165333130](http://cdn.nefu-yzk.top/img/image-20211113165333130.png)

受限于结构，RNN只能实现N-N、1-N、N-1的关系，无法处理N-M的问题

人们找到了SEQ2SEQ模型

该模型拥有一个编码器Encoder和解码器Decoder的模型

Encoder和Decoder依然是RNN网络

先由Encoder提取原始句子的意义，再由Decoder将意义转换为对应的语言

![image-20211113165749620](http://cdn.nefu-yzk.top/img/image-20211113165749620.png)

依靠意义这一中介，seq2seq成功解决了两端单词数不对等的状况

但是意义单元能够存储的信息是有限的，如果一个句子太长，那么翻译的精度也会随之下降，于是人们找到了Attention——注意力机制

在seq2seq的基础结构上，生成每个单词时，都有意识的从原始句子中提取生成该单词时最需要的信息，成功摆脱了输入序列的长度限制

![image-20211113170254608](http://cdn.nefu-yzk.top/img/image-20211113170254608.png)



### Self-Attention

先提取每个单词的意义，再根据生成顺序选取所需要的信息

![image-20211113170423403](http://cdn.nefu-yzk.top/img/image-20211113170423403.png)



### TransFormer总结

![image-20211113165044109](http://cdn.nefu-yzk.top/img/image-20211113165044109.png)

