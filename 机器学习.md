本博客参考[机器学习入门学习](https://www.bilibili.com/video/BV1a7411d7fk)视频进行学习整理，总共学习分为三天，day1的摘要内容：

![image-20210831163321760](http://qynll5cd5.hn-bkt.clouddn.com/img/image-20210831163321760.png)

## 机器学习概要

```apl
  --了解机器学习定义以及应用场景
  -- 说明机器学习算法监督学习与无监督学习的区别
  -- 说明监督学习中的分类、回归特点
  -- 说明机器学习算法目标值的两种数据类型
  -- 说明机器学习(数据挖掘)的开发流程
机器学习和人工智能，深度学习的关系
   -- 机器学习是人工智能的一个实现途径
   -- 深度学习是机器学习的一个方法发展而来(机器学习中的神经网络演变而来)
机器学习的应用场景非常多，可以说渗透到了各个行业领域当中。医疗、航空、教育、物流、电商等等领域的各种场景。
机器学习领域：图像识别、传统预测、NLP(文本分类、情感分析、自动聊天、文本检测等等)
定义：机器学习是从数据中自动分析获得模型，并利用模型对未知数据进行预测。  通过训练数据获得模型，进行预测
```

![image-20210831215046898](http://qynll5cd5.hn-bkt.clouddn.com/img/image-20210831215046898.png)

```apl
人与机器的类比：
  -- 我们是从大量的日常经验中归纳规律，当面临新的问题的时候，就可以利用以往总结的规律去分析现实状况，采取最佳策略。
  -- 机器是从以往的数据中自动分析获得模型，然后可以进行预测分析、辨别物品
数据集的构成： 特征值+目标值
样本即是指每一行数据
没有目标值的数据训练属于无监督学习
```

![image-20210831215610170](http://qynll5cd5.hn-bkt.clouddn.com/img/image-20210831215610170.png)

## 机器学习的分类

![image-20210831215659350](http://qynll5cd5.hn-bkt.clouddn.com/img/image-20210831215659350.png)

```apl
特征值为类别的问题为分类问题，为连续类型的为回归问题
监督学习(supervised learning)(预测):
  -- 定义：输入数据=特征值+目标值。函数的输出可以是一个连续的值(称为回归），或是输出是有限个离散值（称作分类）。
  -- 分类  k-近邻算法、贝叶斯分类、决策树与随机森林、逻辑回归、神经网络
  -- 回归  线性回归、岭回归
无监督学习(unsupervised learning)：
  -- 定义：输入数据是由输入特征值所组成。
  -- 聚类 k-means
机器学习的开发流程：
  -- 获取数据
  -- 数据清洗
  -- 特征过程
  -- 机器学习算法训练->模型
  -- 模型评估
  -- 应用
```

![image-20210831220156939](http://qynll5cd5.hn-bkt.clouddn.com/img/image-20210831220156939.png)

## 特征工程

```apl
 -- 了解特征工程在机器学习当中的重要性
 -- 应用sklearn实现特征预处理
 -- 应用sklearn实现特征抽取
 -- 应用sklearn实现特征选择
 -- 应用PCA实现特征的降维
```

## scikit-learn数据集API介绍

```apl
datasets.load_*()
 -- 获取小规模数据集，数据包含在datasets里
datasets.fetch_*(data_home=None)
 -- 获取大规模数据集，需要从网络上下载，函数的第一个参数是data_home，表示数据集下载的目录,默认是 ~/scikit_learn_data/
sklearn.datasets.load_iris()：加载并返回鸢尾花数据集
sklearn.datasets.load_boston()：加载并返回波士顿房价数据集

load和fetch返回的数据类型datasets.base.Bunch(字典格式)
  -- data：特征数据数组，是 [n_samples * n_features] 的二维 numpy.ndarray 数组
  -- target：标签数组，是 n_samples 的一维 numpy.ndarray 数组
  -- DESCR：数据描述
  -- feature_names：特征名,新闻数据，手写数字、回归数据集没有
  -- target_names：标签名
```

## **数据集划分api**

```apl
sklearn.model_selection.train_test_split(arrays, *options)
  -- x 数据集的特征值
  -- y 数据集的特征值
  -- test_size 测试集的大小，一般为float
  -- random_state 随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。
  -- return 测试集特征训练集特征值值，训练标签，测试标签(默认随机取)
返回的数据依次为x_train, x_test, y_train, y_test
```

## 特征工程

```apl
-- 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。
特征工程是使用专业背景知识和技巧处理数据，使得特征能在机器学习算法上发挥更好的作用的过程。
pandas:一个数据读取非常方便以及基本的处理格式的工具
sklearn:对于特征的处理提供了强大的接口
特征工程包含内容：
  -- 特征抽取:将任意数据（如文本或图像）转换为可用于机器学习的数字特征
   0.特征值化是为了计算机更好的去理解数据
   1.应用DictVectorizer实现对类别特征进行数值化、离散化
   2.应用CountVectorizer实现对文本特征进行数值化
   3.应用TfidfVectorizer实现对文本特征进行数值化
  -- 特征预处理：归一化、标准化
  -- 特征降维：PCA
特征抽取分为：字典特征提取、文本特征提取、图像特征提取
特征提取API：sklearn.feature_extraction

字典特征提取：对字典数据进行特征值化
-- sklearn.feature_extraction.DictVectorizer(sparse=True,…)
  -- DictVectorizer.fit_transform(X) X:字典或者包含字典的迭代器返回值：返回sparse矩阵
  -- DictVectorizer.inverse_transform(X) X:array数组或者sparse矩阵 返回值:转换之前数据格式
  -- DictVectorizer.get_feature_names() 返回类别名称
流程分析：
  -- 实例化类DictVectorizer
  -- 调用fit_transform方法输入数据并转换（注意返回格式）
注：对于特征当中存在类别信息的我们都会做one-hot编码处理，节省资源

文本特征提取：对文本数据进行特征值化
sklearn.feature_extraction.text.CountVectorizer(stop_words=[])   返回词频矩阵
  -- CountVectorizer.fit_transform(X) X:文本或者包含文本字符串的可迭代对象 返回值：返回sparse矩阵
  -- CountVectorizer.inverse_transform(X) X:array数组或者sparse矩阵 返回值:转换之前数据格
  -- CountVectorizer.get_feature_names() 返回值:单词列表
  -- sklearn.feature_extraction.text.TfidfVectorizer

对于中文，先调用jieba进行分析，然后将分词后的结果放进转换器中进行特征提取
jieba分词处理：
  -- jieba.cut()：返回词语组成的生成器

Tf-idf文本特征提取：
TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。
TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。
词频（term frequency，tf）指的是某一个给定的词语在该文件中出现的频率
逆向文档频率（inverse document frequency，idf）是一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到

注：假如一篇文件的总词语数是100个，而词语"非常"出现了5次，那么"非常"一词在该文件中的词频就是5/100=0.05。而计算文件频率（IDF）的方法是以文件集的文件总数，除以出现"非常"一词的文件数。所以，如果"非常"一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是lg（10,000,000 / 1,0000）=3。最后"非常"对于这篇文档的tf-idf的分数为0.05 * 3=0.15
```

![image-20210831222016547](http://qynll5cd5.hn-bkt.clouddn.com/img/image-20210831222016547.png)

## 特征预处理

```apl
-- 应用MinMaxScaler实现对特征数据进行归一化
-- 应用StandardScaler实现对特征数据进行标准化
特征预处理API:sklearn.preprocessing
预处理是为了进行无量纲化，使不同规格的数据转换到同一规格
sklearn.preprocessing.MinMaxScaler (feature_range=(0,1)… )
归一化：最大值最小值是变化的，另外，最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。
标准化相比较于归一化而言：
对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变
对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。
标准化：在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。
```

## 特征降维

```apl
-- 知道特征选择的嵌入式、过滤式以及包裹氏三种方式
-- 应用VarianceThreshold实现删除低方差特征
-- 了解相关系数的特点和计算 皮尔逊相关系数
-- 应用相关性系数实现特征选择

降维是指在某些限定条件下，降低随机变量(特征)个数，得到一组“不相关”主变量的过程 降低随机变量的个数
特征选择：数据中包含冗余或无关变量（或称特征、属性、指标等），旨在从原有特征中找出主要特征。
方法：
-- Filter(过滤式)：主要探究特征本身特点、特征与特征和目标值之间关联
   -- 方差选择法：低方差特征过滤
   -- 相关系数
-- Embedded (嵌入式)：算法自动选择特征（特征与目标值之间的关联）
   -- 决策树:信息熵、信息增益
   -- 正则化：L1、L2
   -- 深度学习：卷积等
调用的模块：sklearn.feature_selection
低方差特征过滤：删除低方差的一些特征，前面讲过方差的意义。再结合方差的大小来考虑这个方式的角度。
  -- 特征方差小：某个特征大多样本的值比较相近
  -- 特征方差大：某个特征很多样本的值都有差别
```

## 主成分分析

```apl
-- 应用PCA实现特征的降维
-- 用户与物品类别之间主成分分析
定义：高维数据转化为低维数据的过程，在此过程中可能会舍弃原有数据、创造新的变量
作用：是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。
应用：回归分析或者聚类分析当中
API：sklearn.decomposition.PCA(n_components=None)
  -- 将数据分解为较低维数空间
  -- n_components:
     小数：表示保留百分之多少的信息
     整数：减少到多少特征
  -- PCA.fit_transform(X) X:numpy array格式的数据[n_samples,n_features]
  -- 返回值：转换后指定维度的array
```

## 总结

![image-20210831222944404](http://qynll5cd5.hn-bkt.clouddn.com/img/image-20210831222944404.png)

主要的课堂代码以及相关的注解：

```python
# load_*  导入较小的数据集  fetch_*  导入较大的数据集
from sklearn.datasets import load_iris
#模型选择，数据集划分 返回值依次为x_train, x_test, y_train, y_test
from sklearn.model_selection import train_test_split
#feature_extraction 特征提取， DictVectorizer字典特征抽取  适用于分类较多或者是字典的数据
from sklearn.feature_extraction import DictVectorizer
#CountVectorizer, TfidfVectorizer 对文本特征进行提取
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
#数据进行处理，分为标准化和归一化(容易受到异常值影响)，一般情况下使用标准化进行处理，避免大数吃小数和量纲的影响
from sklearn.preprocessing import MinMaxScaler, StandardScaler
#VarianceThreshold 低方差法，过滤掉一些普遍的特征，比如鸟的羽毛和爪子，所有的鸟都有羽毛和爪子，因此可以不作为特征
from sklearn.feature_selection import VarianceThreshold
from sklearn.decomposition import PCA
#pearsonr 皮尔逊相关系数，在[-1, 1]之间，分为正相关和负相关1
from scipy.stats import pearsonr
import jieba
import pandas as pd


def datasets_demo():
    """
    sklearn数据集的使用
    :return:
    """
    #获取数据集
    iris = load_iris()
    #获得的是Bunch数据，字典的形式  可以通过iris[”key“]  iris.key
    print("鸢尾花数据集：\n", iris)
    print("查看数据集描述：\n", iris["DESCR"])  #iris["DESCR"] <--> iris.DESCR
    print("查看特征值的名字：\n", iris.feature_names)  #iris["feature_names"]  <--> iris.feature_names
    print("查看特征值：\n", iris.data, iris.data.shape)

    #数据集的划分：测试集和训练集
    x_tarin, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=22)
    print("训练集的特征值: \n", x_tarin, x_tarin.shape)
    return None

def dict_demo():
    """
    字典特征抽取
    :return:
    """
    data = [{'city': '北京','temperature':100}, {'city': '上海','temperature':60}, {'city': '深圳','temperature':30}]

    #实例化一个字典特征抽取转换器类，适用于分类较多的或者是本身是字典类型的数据
    transfer = DictVectorizer()  #或者其中加参数

    #调用fit_transform()
    data_new = transfer.fit_transform(data)
    print("data_new:\n, ", data_new)  #默认是稀疏矩阵存储   data_new.toarray() 转成矩阵存储
    print("特征名字：\n", transfer.get_feature_names())
    return None

def count_demo():
    """
    文本特征抽取：CountVectorizer
    :return:
    """
    data = ["life is short,i like like python", "life is too long,i dislike python"]
    #获得CountVectorizer转换器实例
    transfer = CountVectorizer(stop_words=["is", "too"])  #stop_words去掉一些词，不用在分词中
    data_new = transfer.fit_transform(data)
    print("data_new:\n", data_new)
    print("特征名字:\n", transfer.get_feature_names())
    return None


def cut_word(text):
    """
    进行中文分词："我爱北京天安门" --> "我 爱 北京 天安门"
    :param text:
    :return:
    """
    return " ".join(list(jieba.cut(text)))

def count_chinese_demo():
    """
    中文文本特征抽取，自动分词
    :return:
    """
    # 将中文文本进行分词
    data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",
            "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
            "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]

    data_new = []
    for sent in data:
        data_new.append(cut_word(sent))
    # print(data_new)
    # 1、实例化一个转换器类
    transfer = CountVectorizer()

    #调用fit_transform进行数据转换
    #特征提取即是将任意数据（如文本或图像）转换为可用于机器学习的数字特征
    data_final = transfer.fit_transform(data_new)
    print("data_final:\n", data_final)
    print("特征名称\n", transfer.get_feature_names())
    return None

def tfdif_demo():
    data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",
            "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
            "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]

    data_new = []
    for sent in data:
        data_new.append(cut_word(sent))
    #实例化一个转换器类
    transfer = TfidfVectorizer()

    #调用fit_transform进行数据转换
    data_final = transfer.fit_transform(data_new)
    print("data_final:\n", data_final)
    print("特征名称\n", transfer.get_feature_names())
    return None

def minmax_demo():
    data = pd.read_csv("dating.txt")

    #iloc 参数1表示所有行，参数二表示1到3列
    data = data.iloc[:, :3]
    # print(data)
    #实例化一个转换器类
    #归一化处理，避免大数吃小数的问题
    transfer = MinMaxScaler()

    #调用fit_transfrom进行数据转换
    data_new = transfer.fit_transform(data)
    print("data_new:\n", data_new)
    return None

def stand_demo():
    #获取数据
    #实例化一个转换器类，调用fit_transfrom进行数据转换
    #然后获取数据，转换的数据格式是与实例的转换器类息息相关的
    data = pd.read_csv("dating.txt")

    data = data.iloc[:, :3]

    transfer = StandardScaler()
    data_new = transfer.fit_transform(data)
    print("data_new:\n", data_new)
    return None

def variance_demo():
    """
    过滤低方差的特征，相当于过滤掉普遍都有的特征，例如鸟的羽毛和爪子
    :return:
    """
    #获取数据
    data = pd.read_csv("factor_returns.csv")
    data = data.iloc[:, 1:-2]  #表示要全部行，然后从第一列到倒数第二列  序号是从0开始的
    print("data:\n", data)
    #实例化一个转换器类
    transfer = VarianceThreshold(threshold=10)  #threshold方差过滤的门槛，默认是0，自行调整
    data_new = transfer.fit_transform(data)
    print("data_new:\n", data_new, data_new.shape)
    #引入皮尔逊相关系数，计算两列的相关性
    r1 = pearsonr(data["pe_ratio"], data["pb_ratio"])
    print("相关系数：\n", r1)
    r2 = pearsonr(data['revenue'], data['total_expense'])
    print("revenue与total_expense之间的相关性：\n", r2)
    return None

def pca_demo():
    """
    PCA降维
    :return:
    """
    data = [[2,8,4,5], [6,3,0,8], [5,4,9,1]]

    # 1、实例化一个转换器类
    # n_components  小数表示保留的信息程度
    # n_components  整数表示保留的特征个数，一般是采用浮点数
    transfer = PCA(n_components=0.95)

    # 2、调用fit_transform
    data_new = transfer.fit_transform(data)
    print("data_new:\n", data_new)
    return None


if __name__ == '__main__':
    # 代码1：sklearn数据集使用
    # datasets_demo()
    # 代码2：字典特征抽取
    # dict_demo()
    # 代码3：文本特征抽取：CountVecotrizer
    # count_chinese_demo()
    # 代码4：中文分词
    # print(cut_word("我爱北京天安门"))
    # 代码5：用TF-IDF的方法进行文本特征抽取
    # tfidf_demo()
    # 代码6：归一化
    # minmax_demo()
    # 代码7：标准化
    # stand_demo()
    # 代码8：低方差特征过滤
    # variance_demo()
    # 代码9：PCA降维
    pca_demo()
```

PCA的一个例子：

```python
# 1、获取数据
# 2、合并表
# 3、找到user_id和aisle之间的关系
# 4、PCA降维
import pandas as pd
# 1、获取数据
order_products = pd.read_csv("./instacart/order_products__prior.csv")
products = pd.read_csv("./instacart/products.csv")
orders = pd.read_csv("./instacart/orders.csv")
aisles = pd.read_csv("./instacart/aisles.csv")

# 2、合并表
# order_products__prior.csv：订单与商品信息

# 字段：order_id, product_id, add_to_cart_order, reordered
# products.csv：商品信息
# 字段：product_id, product_name, aisle_id, department_id
# orders.csv：用户的订单信息
# 字段：order_id,user_id,eval_set,order_number,….
# aisles.csv：商品所属具体物品类别
# 字段： aisle_id, aisle

# 合并aisles和products aisle和product_id
tab1 = pd.merge(aisles, products, on=["aisle_id", "aisle_id"])
tab2 = pd.merge(tab1, order_products, on=["product_id", "product_id"])
tab3 = pd.merge(tab2, orders, on=["order_id", "order_id"])
tab3.head()  #head函数可以默认读取前5行的数据

# 3、找到user_id和aisle之间的关系
table = pd.crosstab(tab3["user_id"], tab3["aisle"])   #表示user_id为行  aisle为列
# 1）实例化一个转换器类   表示减少特征的情况下需要保证95%以上的信息
transfer = PCA(n_components=0.95)

# 2）调用fit_transform进行数据转换
data_new = transfer.fit_transform(data)
data_new.shape
```

